{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hi there!","text":"<p>I'm a Chief Architect at Red Hat working in the Asia Pacific CTO Office.</p> <p>My background is as an applied researcher who stumbled into the world of software  via MATLAB and Fortran. I have a masochistic streak which draws me to problems where business/mission complexity requires innovative solutions. If those solutions have an element of supporting good governance even better!</p> <p>Today I'm refining my mission at Red Hat. Interesting things I've been working on so far include:</p> <ul> <li>GPUaaS infrastructures with partners</li> <li>Digital Sovereignty including modern cloud infrastructure patterns</li> <li>Enabling AI stacks in a variety of circumstances.</li> <li>Security and Compliance in the complexity of the environments above.</li> </ul> <p>If any of this sounds interesting please reach out via LinkedIn.</p>"},{"location":"#projects-im-actively-working-on","title":"Projects I'm actively working on","text":"<ul> <li>Validated patterns demos - demo scenarios for work running on openshift</li> <li>mdformat fronmatter - formatting utility</li> </ul>"},{"location":"awesome_stuff/","title":"Awesome stuff","text":""},{"location":"awesome_stuff/#about-this-page","title":"About this page","text":"<p>A small list of awesome things that I have found along my techy journey.</p>"},{"location":"awesome_stuff/#command-line-tools","title":"Command line tools","text":"<ul> <li>Brew - missing package manager for mac os</li> <li>Direnv - how could I live without after finding it, contextual credentials based on terminal directory</li> </ul>"},{"location":"awesome_stuff/#documentation-tools","title":"Documentation tools","text":"<ul> <li>Mkdocs - python based documentation generation</li> <li>Mkdocs Material - ever expanding theme</li> </ul>"},{"location":"awesome_stuff/#linting","title":"Linting","text":"<ul> <li><code>pre-commit</code></li> <li><code>mdformat</code></li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2023/02/01/documentation-as-code/","title":"Documentation as code","text":"<p>AUCloud has relaunched it's documentation site from a closed forum, to an open source website managed as code. Read more about the technology and how the team I worked with  managed it using GitHub and GitHub actions.</p>"},{"location":"blog/2023/02/01/documentation-as-code/#attribution","title":"Attribution","text":"<p>I wrote this blog article originally on AUCloud's Documentation site, with the source format here. This article therefore is licensed under CC-by-SA 4.0.</p>"},{"location":"blog/2023/02/01/documentation-as-code/#improving-client-experience-using-open-source","title":"Improving client experience using open source","text":"<p>It still surprises me that some enterprise software products do not provide their documentation openly on the internet. As a technologist, when evaluating technology products, whether open source or proprietary, having a robust understanding of the breadth and depth of documentation (and ease of access) is crucial.</p> <p>AUCloud has recently relaunched it's documentation site as a static website, built from a GitHub repository. A number of the hyperscalers use a similar method for their documentation as:</p> <ul> <li>The open source documentation core allows users to propose changes to the documentation.</li> <li>Documentation hosted on GitHub becomes searchable within GitHub, embedding it within the workspace of developers.</li> </ul> <p>AUCloud has taken this approach, and extended it to be based on a static-website generator: <code>mkdocs</code>. Using static website generation provides us some clear advantages:</p> <ul> <li>The static website provides a lower-risk vehicle for hosting the documentation from a security point of view;</li> <li>Managing documentation 'as code' allows documentation to be easily integrated in the 'definition of done' in our agile development life cycle;</li> <li>This includes peer review processes implicit in code development platforms.</li> <li>The CI/CD process allows pre-flight checks to be integrated such as spell checking and link validation;</li> <li>We can build with and contribute back to the extended <code>mkdocs</code> ecosystem (such as <code>Material for mkdocs</code>)</li> </ul> <p>The result for AUCloud is an documentation site that is fast, easy to maintain, easily searchable and easily updatable.</p>"},{"location":"blog/2023/02/01/documentation-as-code/#continuous-documentation-integration-and-continuous-documentation-delivery-cdicdd","title":"Continuous Documentation Integration and Continuous Documentation Delivery (CDI/CDD)","text":"<p>Our DevOps workflows have to provide functionality for two scenarios:</p> <ol> <li>The external repository where:</li> <li>There is only one version of the website (the current <code>main</code> branch).</li> <li>External contributions, which are assumed to be \"small\" must be validated.</li> <li>A private repository where:</li> <li>AUCloud employees may need to stage larger contributions (e.g. for an upcoming product launch).<ol> <li>Implicitly this implies providing previews of the staged version of the website.</li> </ol> </li> </ol> <p>We serve these from a common repository that is synchronized across our internal GitHub Enterprise Server, and the <code>github.com</code> repository. On both of these platforms GitHub pages, is the mechanism for serving internal and external copies of the documentation site.</p> <p>GitHub vs GitHub Enterprise Server</p> <p>Our private repository is hosted on a GitHub Enterprise Server appliance, however, there is no reason why both the private and public repositories could not be hosted on github.com.</p> <p>On the private repository individual contributors can build their own version of the documentation by simply pushing content to a branch. <code>mike</code>, is a complementary tool to MkDocs and Material for MkDocs which provides versioning support. Typically mike is used to support multiple versions of software products. In this case, mike is used to generate a version of the website, per branch pushed to our private repository. On the public repository, this functionality is disabled.</p> <p>Deletion of a branch automatically triggers cleanup up the branch build.</p> <p>Pushing to the main branch on the private repository triggers a website release and a corresponding update on our public repository.</p> <p>The result is that the website can be updated every 5-10 minutes. Currently the largest single time consumer in the build process is validation of external and internal links. While it's slightly slow, we believe it is necessary as it gives our teams early warning when external (or internal) parties are updating their documentation (and we should too!).</p>"},{"location":"blog/2023/02/01/documentation-as-code/#challenges","title":"Challenges","text":"<p>Two significant challenges were faced building these pipelines:</p> <ul> <li>Managing race conditions on the  <code>gh-pages</code> branch.</li> <li>Bi-directional synchronization of git repositories.</li> </ul>"},{"location":"blog/2023/02/01/documentation-as-code/#managing-race-conditions-on-the-gh-pages-branch","title":"Managing race conditions on the  <code>gh-pages</code> branch","text":"<p>One gap that we've observed since adopting github actions, is to be careful for introducing any race conditions / circular workflows. Github actions does expose concurrency control mechanisms. However, when dealing with branch based builds of the website we needed <code>exactly once</code> semantics for adding and removing the branch releases. The challenge is you want to remove a branch based build, just as you are updating the release version of the website, and all of these are on the <code>gh-pages</code> branch.</p> <p>As a result we experienced frequent contention issues. Our approach to date is brute force, however, newer features, such as merge-queues will decrease risk when available.</p>"},{"location":"blog/2023/02/01/documentation-as-code/#bi-directional-synchronization-of-git-repositories","title":"Bi-directional synchronization of git repositories","text":"<p>Due to the multi-repository nature of the pipeline that has been built, the biggest issue is on state synchronization. Force-pushes can be used to mitigate most problems automatically, however, risk removing contribution history from external parties. Current approach taken requires a semi-automated process where pull requests from external contributors need to be merged twice: Once on the public repository; once on the private repository.</p>"},{"location":"blog/2023/02/01/documentation-as-code/#learn-more","title":"Learn more","text":"<ul> <li>Check out the github actions CICD workflows</li> <li>Read more about <code>MkDocs</code> and <code>Material for MkDocs</code></li> <li>Learn more about <code>mike</code> for versioning MkDocs</li> </ul>"},{"location":"blog/2000/01/01/hello-world/","title":"Hello world!","text":"<p>A simple test post as I am bringing this blog alive.</p> <p>Extra content that should not be seen on the head page.</p>"},{"location":"blog/2023/10/30/validated-patterns-for-demos/","title":"Validated patterns for demos","text":"<p>Upon joining Red Hat as a Chief Architect I've been lucky enough to find some time to deep-dive into our products and upstream. One of the fun challenges that I faced straight away is our demo OpenShift environments, for many reasons, are emphemeral.</p> <p>In ephemeral environments GitOps is essential to quickly get to a consistent environment. Enter validated patterns.</p> <p>At a high level validated patterns gitops based reference architectures for multiple OpenShift clusters running across multiple environments (both on prem and in the cloud).</p> <p>The core <code>multicloud-gitops</code> provides:</p> <ul> <li>Multicluster management using Red Hat Advanced Cluster Manager,</li> <li>A hub-and-spoke approach to secrets management with upstream Vault Project,</li> <li>A single point for managing both operators and argo applications,</li> <li>Hooks for the procedural steps you just can't get rid of (such as unsealing vault).</li> </ul> <p>The result from an operationl perspective once I have a cluster the only steps I care about are:</p> <ol> <li>Cloning a pattern repo - <code>git clone git@github.com/butler54/validated-patterns-demos</code>,</li> <li>Logging into the cluster - <code>oc login --token=sha256~*** --server=https://URL:PORT</code>,</li> <li>Setup required secrets using the secret-template, if required;</li> <li><code>./pattern.sh make install</code>.</li> </ol> Podman and bootstrapping on mac os <p>To run the bootstrap scripts validated patterns presumes that you are using <code>podman</code>. Using brew:</p> <pre><code>brew install podman\npodman machine init\npodman machine start\n</code></pre> <p>All I have to do is wait and the environment will roll itself out automatically. Using OpenShift GitOps (Argo CD) the deployment rollout can easily be monitored and triaged for errors.</p> <p>When developing my own pattern or your own the best way to start is to fork from the <code>multicloud-gitops</code> repository.</p> <p>The validated-patterns operator has a few nice features such as branch based deployments but the setup has two features that I found to be essential for demo / development environments</p> <ul> <li>Safe by default secrets loading;</li> <li>Environmental overrides.</li> </ul>"},{"location":"blog/2023/10/30/validated-patterns-for-demos/#safe-by-default-secrets-loading","title":"Safe by default secrets loading","text":"<p>While in production environments enterprises typically take great care with secrets, in early stage development it is not uncommon for developers to be manipulating secrets from their laptops. A nice feature of validated patterns sit that the secrets bootstrapping when calling <code>./pattern.sh make install</code> and <code>./pattern.sh make load-secrets</code> explicitly does not look in the cloned source repo.</p> <p>Instead it looks for files in a users home directory which is highly unlikely to be managed by git. The result is that the template helps decrease the risk of developers committing secrets - as there is no reason to have the secrets in the repository at any point in time.</p>"},{"location":"blog/2023/10/30/validated-patterns-for-demos/#environmental-overrides","title":"Environmental overrides","text":"<p>The <code>values.yaml</code> files provides the high level abstraction of what needs to be deployed onto a cluster. For example <code>values-hub.yaml</code> example below deploys ACM, OpenShift pipelines (Tekton) and a pipelines Helm chart that contains Tekton pipeline definitions.</p> <pre><code>clusterGroup:\n  name: hub\n  isHubCluster: true\n\n  namespaces:\n    - open-cluster-management\n    - vault\n    - golang-external-secrets\n    - devops\n  # Operator subscriptions\n  subscriptions:\n    acm:\n      name: advanced-cluster-management\n      namespace: open-cluster-management\n      channel: release-2.8\n\n    openshift-pipelines:\n      name: openshift-pipelines-operator-rh\n      namespace: openshift-operators\n  # OCP project\n  projects:\n    - hub\n    - devops\n\n  # defining the path to an argoCD application (e.g. helm / Kustomize)\n  applications:\n    acm:\n      name: acm\n      namespace: open-cluster-management\n      project: hub\n      path: common/acm\n      ignoreDifferences:\n        - group: internal.open-cluster-management.io\n          kind: ManagedClusterInfo\n          jsonPointers:\n            - /spec/loggingCA\n\n\n    pipelines:\n      name: pipelines\n      namespace: devops\n      project: devops\n      path: charts/all/pipelines\n      ignoreDifferences:\n        - kind: ServiceAccount\n          jsonPointers:\n            - /imagePullSecrets\n            - /secrets\n  sharedValueFiles:\n    - /overrides/values-{{ $.Values.global.clusterPlatform }}.yaml\n    - /overrides/values-{{ $.Values.global.clusterPlatform }}-{{ $.Values.global.clusterVersion\n      }}.yaml\n</code></pre> <p>In the case of my pipelines it requests a <code>storageclass</code> by name in the pipelines Helm chart. <code>storageclasses</code> typically have different names across different cloud providers which means we need provider specific overrides. The validated patterns operator provides a framework where a combination of <code>clusterGroup</code>, cloud provider and OpenShift version. This gives you value files of the format:</p> <ul> <li><code>values-global.yaml</code> default applies everywhere,</li> <li><code>values-hub.yaml</code> hub for your 'hub' RHACM cluster,</li> <li><code>sharedValueFiles</code></li> </ul> <p><code>sharedValueFiles</code> is an list defined in <code>values-hub.yaml</code> where you can define an ordered list by which files are overload the other values files. Within this environment we can use contextual information about the cluster such as:</p> <ul> <li><code>clusterGroup</code> - a user provided label.</li> <li><code>clusterPlatform</code> e.g. AWS, Azure, IBMCloud</li> <li><code>clusterVersion</code> e.g. 4.13</li> </ul> <p>to pick up override files if the exist.</p> clusterGroup <p><code>clusterGroup</code> is a label used together with RHACM particularly for clusters beyond the first. Applying:</p> <p><code>oc label managedclusters.cluster.open-cluster-management.io/&lt;your-cluster&gt; clusterGroup=&lt;managed-cluster-group&gt;</code></p> <p>will result in the correct <code>clusterGroup</code> payload being applied ot a given cluster.</p> <p>In this case my <code>pipelines</code> Helm chart presumes that the storageclass is defined in <code>{{ .Values.cloudProvider.storageClass }}</code> so to setup for both IBM Cloud and AWS using the <code>sharedValuesFiles</code> defined in the example yaml file above I created two files to contain the overrides:</p> <p><code>overrides/values-AWS.yaml</code></p> <pre><code>cloudProvider:\n  storageClass: gp3-csi\n</code></pre> <p><code>overrides/values-IBMCloud.yaml</code></p> <pre><code>cloudProvider:\n  storageClass: ibmc-vpc-block-10iops-tier\n</code></pre> <p>These override any values in the Helm chart's default <code>Values.yaml</code> file.</p>"},{"location":"blog/2023/10/30/validated-patterns-for-demos/#wrap-up","title":"Wrap up","text":"<p>Validated patterns, together with Helm, Argo CD, and RHACM. Provides a powerful tool to achieve consistency across multiple clusters and clouds.</p> <p>This blog was written based on this version of my validated-demos repo.</p> <p>Thanks to @beekhof and @day0hero who spent a considerable amount of time teaching me (and others) about validated patterns.</p>"},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2000/","title":"2000","text":""},{"location":"blog/category/validated-patterns/","title":"validated-patterns","text":""},{"location":"blog/category/gitops/","title":"gitops","text":""},{"location":"blog/category/openshift/","title":"openshift","text":""},{"location":"blog/category/demos/","title":"demos","text":""},{"location":"blog/category/mkdocs/","title":"mkdocs","text":""},{"location":"blog/category/documentation/","title":"documentation","text":""},{"location":"blog/category/cicd/","title":"CICD","text":""},{"location":"blog/category/github/","title":"GitHub","text":""},{"location":"blog/category/hello/","title":"Hello","text":""},{"location":"blog/category/world/","title":"World","text":""}]}